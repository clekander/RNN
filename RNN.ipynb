{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52826d40",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks\n",
    "\n",
    "Gabs DiLiegro, London Kasper, Carys LeKander\n",
    "\n",
    "Dataset: https://www.kaggle.com/datasets/uciml/sms-spam-collection-dataset?select=spam.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565fb1a1",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "\n",
    "Our dataset is a collection of text messages that are identified as normal or spam texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51360b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5572 entries, 0 to 5571\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   v1      5572 non-null   object\n",
      " 1   v2      5572 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 87.2+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('spam.csv',encoding='latin-1')\n",
    "df = df.drop(['Unnamed: 2','Unnamed: 3', 'Unnamed: 4'], axis=1)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa83598b",
   "metadata": {},
   "source": [
    "The first column of our dataset identifies if it is a spam text. We used label encoding below to change the column to be 0 for a normal text and 1 for a spam text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03f7e0c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   v1                                                 v2\n",
       "0   0  Go until jurong point, crazy.. Available only ...\n",
       "1   0                      Ok lar... Joking wif u oni...\n",
       "2   1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   0  U dun say so early hor... U c already then say...\n",
       "4   0  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(df['v1'])\n",
    "df['v1'] = le.transform(df['v1'])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0af1766",
   "metadata": {},
   "source": [
    "Looking at the totals for each category, we can see that there are many more normal text messages than spam messages in our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5991375",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4825\n",
       "1     747\n",
       "Name: v1, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totals = df['v1'].value_counts()\n",
    "totals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fbb2d2",
   "metadata": {},
   "source": [
    "The longest text message in our dataset is 171 which we will use for our max length during tokenization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0e71155",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "171"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_words = 0\n",
    "for i in df.v2:\n",
    "    if len(i.split()) > max_words:\n",
    "        max_words = len(i.split())\n",
    "max_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6187c0",
   "metadata": {},
   "source": [
    "For our tokenization, we converted each word to an integer using the Keras Tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06fa4d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-13 17:17:37.725877: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8920 unique tokens. Distilled to 8920 top words.\n",
      "Shape of data tensor: (5572, 171)\n",
      "CPU times: user 7.59 s, sys: 1.38 s, total: 8.97 s\n",
      "Wall time: 11.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "NUM_TOP_WORDS = None # use entire vocabulary!\n",
    "MAX_ART_LEN = max_words # maximum and minimum number of words\n",
    "\n",
    "#tokenize the text\n",
    "tokenizer = Tokenizer(num_words=NUM_TOP_WORDS)\n",
    "tokenizer.fit_on_texts(df.v2)\n",
    "# save as sequences with integers replacing words\n",
    "sequences = tokenizer.texts_to_sequences(df.v2)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "NUM_TOP_WORDS = len(word_index) if NUM_TOP_WORDS==None else NUM_TOP_WORDS\n",
    "top_words = min((len(word_index),NUM_TOP_WORDS))\n",
    "print('Found %s unique tokens. Distilled to %d top words.' % (len(word_index),top_words))\n",
    "\n",
    "X = pad_sequences(sequences, maxlen=MAX_ART_LEN)\n",
    "print('Shape of data tensor:', X.shape)\n",
    "y = df.v1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd61fa95",
   "metadata": {},
   "source": [
    "For our business case, we want to notify users when they recieve a text that may be spam. If we misclassify something as spam, the user may be slightly annoyed. However, if we do not clasify something as spam when it is, it could lead to the user trusting a fraudulent text. Therefore we want to minimize the number of false negative (number of spam texts classified as normal texts), so we will use recall to measure our algorithm's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab58da1a",
   "metadata": {},
   "source": [
    "We chose to use a Stratified 10-fold cross validation for dividing our data into training and testing. We chose this method because our dataset is smaller (5572) and there are not many examples of spam texts (747). We want to make sure that we represent the entire dataset in equal proportion.\n",
    "\n",
    "MAYBE ADD MORE EXPLANATION TO THIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15b083d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=1).split(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542cad62",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ca36fa",
   "metadata": {},
   "source": [
    "***Investigate at least two different recurrent network architectures (perhaps LSTM and GRU). Alternatively, you may also choose one recurrent network and one convolutional network. Be sure to use an embedding layer (try to use a pre-trained embedding, if possible). Adjust hyper-parameters of the networks as needed to improve generalization performance (train a total of at least four models). Discuss the performance of each network and compare them.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac783846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n",
      "Embedding Shape: (8921, 100) \n",
      " Total words found: 6518 \n",
      " Percentage: 73.06355789709674\n",
      "CPU times: user 17.2 s, sys: 1.08 s, total: 18.3 s\n",
      "Wall time: 20.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "EMBED_SIZE = 100\n",
    "# the embed size should match the file you load glove from\n",
    "embeddings_index = {}\n",
    "f = open('large_data/glove.6b/glove.6B.100d.txt')\n",
    "# save key/array pairs of the embeddings\n",
    "#  the key of the dictionary is the word, the array is the embedding\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "# now fill in the matrix, using the ordering from the\n",
    "#  keras word tokenizer from before\n",
    "found_words = 0\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBED_SIZE))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be ALL-ZEROS\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        found_words = found_words+1\n",
    "\n",
    "print(\"Embedding Shape:\",embedding_matrix.shape, \"\\n\",\n",
    "      \"Total words found:\",found_words, \"\\n\",\n",
    "      \"Percentage:\",100*found_words/embedding_matrix.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7be6d1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Input, LSTM, GRU\n",
    "from tensorflow.keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "491869b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save this embedding now\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBED_SIZE,\n",
    "                            weights=[embedding_matrix],# here is the embedding getting saved\n",
    "                            input_length=MAX_ART_LEN,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2e3a3f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 171, 100)          892100    \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 100)               80400     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 972,601\n",
      "Trainable params: 80,501\n",
      "Non-trainable params: 892,100\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "lstm1 = Sequential()\n",
    "lstm1.add(embedding_layer)\n",
    "lstm1.add(LSTM(100,dropout=0.2, recurrent_dropout=0.2))\n",
    "lstm1.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "print(lstm.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bfb8e2e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 171, 100)          892100    \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 100)               80400     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 972,601\n",
      "Trainable params: 80,501\n",
      "Non-trainable params: 892,100\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "lstm2 = Sequential()\n",
    "lstm2.add(embedding_layer)\n",
    "lstm2.add(LSTM(100,dropout=0.2, recurrent_dropout=0.3))\n",
    "lstm2.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "print(lstm2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ac83a9d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 171, 100)          892100    \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 100)               80400     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 972,601\n",
      "Trainable params: 80,501\n",
      "Non-trainable params: 892,100\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "gru1 = Sequential()\n",
    "gru1.add(embedding_layer)\n",
    "gru1.add(GRU(100,dropout=0.2, recurrent_dropout=0.2))\n",
    "gru1.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "print(gru.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "69f3a466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 171, 100)          892100    \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 100)               80400     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 972,601\n",
      "Trainable params: 80,501\n",
      "Non-trainable params: 892,100\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "gru2 = Sequential()\n",
    "gru2.add(embedding_layer)\n",
    "gru2.add(GRU(100,dropout=0.2, recurrent_dropout=0.3))\n",
    "gru2.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "print(gru.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c2e79d",
   "metadata": {},
   "source": [
    "***Use the method of train/test splitting and evaluation criteria that you argued for at the beginning of the lab. Visualize the results of all the RNNs you trained.  Use proper statistical comparison techniques to determine which method(s) is (are) superior.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dd3e7c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Epoch 1/3\n",
      "63/63 [==============================] - 43s 606ms/step - loss: 0.0784 - accuracy: 0.9756 - recall: 0.8832 - val_loss: 0.0512 - val_accuracy: 0.9843 - val_recall: 0.8833\n",
      "Epoch 2/3\n",
      "63/63 [==============================] - 36s 571ms/step - loss: 0.0615 - accuracy: 0.9793 - recall: 0.8978 - val_loss: 0.0391 - val_accuracy: 0.9888 - val_recall: 0.9500\n",
      "Epoch 3/3\n",
      "63/63 [==============================] - 37s 583ms/step - loss: 0.0593 - accuracy: 0.9791 - recall: 0.8996 - val_loss: 0.0443 - val_accuracy: 0.9910 - val_recall: 0.9667\n",
      "Score for fold 1: loss of 0.04433740675449371; accuracy of 99.10112619400024%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Epoch 1/3\n",
      "63/63 [==============================] - 43s 609ms/step - loss: 0.0580 - accuracy: 0.9831 - recall_1: 0.9124 - val_loss: 0.0229 - val_accuracy: 0.9978 - val_recall_1: 0.9833\n",
      "Epoch 2/3\n",
      "63/63 [==============================] - 40s 642ms/step - loss: 0.0483 - accuracy: 0.9853 - recall_1: 0.9270 - val_loss: 0.0201 - val_accuracy: 0.9933 - val_recall_1: 0.9500\n",
      "Epoch 3/3\n",
      "63/63 [==============================] - 60s 954ms/step - loss: 0.0439 - accuracy: 0.9870 - recall_1: 0.9270 - val_loss: 0.0269 - val_accuracy: 0.9955 - val_recall_1: 0.9667\n",
      "Score for fold 2: loss of 0.02687457576394081; accuracy of 99.55056309700012%\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Epoch 1/3\n",
      "63/63 [==============================] - 64s 708ms/step - loss: 0.0374 - accuracy: 0.9883 - recall_2: 0.9360 - val_loss: 0.0223 - val_accuracy: 0.9910 - val_recall_2: 0.9508\n",
      "Epoch 2/3\n",
      "63/63 [==============================] - 45s 718ms/step - loss: 0.0350 - accuracy: 0.9898 - recall_2: 0.9488 - val_loss: 0.0228 - val_accuracy: 0.9865 - val_recall_2: 0.9344\n",
      "Epoch 3/3\n",
      "63/63 [==============================] - 60s 963ms/step - loss: 0.0309 - accuracy: 0.9898 - recall_2: 0.9488 - val_loss: 0.0270 - val_accuracy: 0.9910 - val_recall_2: 0.9344\n",
      "Score for fold 3: loss of 0.027034448459744453; accuracy of 99.10112619400024%\n"
     ]
    }
   ],
   "source": [
    "# Merge inputs and targets\n",
    "inputs = np.concatenate((X_train, X_test), axis=0)\n",
    "targets = np.concatenate((y_train, y_test), axis=0)\n",
    "\n",
    "acc_per_fold = []\n",
    "recall_per_fold = []\n",
    "loss_per_fold = []\n",
    "\n",
    "# Define the K-fold Cross Validator\n",
    "\n",
    "# K-fold Cross Validation model evaluation\n",
    "fold_no = 1\n",
    "for k, (train, test) in enumerate(kfold):\n",
    "\n",
    "    # Compile the model\n",
    "    lstm.compile(loss='binary_crossentropy',\n",
    "            optimizer='rmsprop',\n",
    "            metrics=['accuracy', keras.metrics.Recall()])\n",
    "\n",
    "\n",
    "    # Generate a print\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "    # Fit data to model\n",
    "    history = lstm.fit(inputs[train], targets[train], epochs=3, batch_size=64, validation_data=(inputs[test], targets[test],))\n",
    "\n",
    "    # Generate generalization metrics\n",
    "    scores = lstm.evaluate(inputs[test], targets[test], verbose=0)\n",
    "    print(f'Score for fold {k}: {lstm.metrics_names[0]} of {scores[0]}; {lstm.metrics_names[1]} of {scores[1]*100}%')\n",
    "    acc_per_fold.append(scores[1] * 100)\n",
    "    loss_per_fold.append(scores[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551a2cc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
