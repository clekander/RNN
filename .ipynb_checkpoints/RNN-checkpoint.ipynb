{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52826d40",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks\n",
    "\n",
    "Gabs DiLiegro, London Kasper, Carys LeKander\n",
    "\n",
    "Dataset: https://www.kaggle.com/datasets/uciml/sms-spam-collection-dataset?select=spam.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565fb1a1",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "\n",
    "Our dataset is a collection of text messages that are identified as normal or spam texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51360b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5572 entries, 0 to 5571\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   v1      5572 non-null   object\n",
      " 1   v2      5572 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 87.2+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('spam.csv',encoding='latin-1')\n",
    "df = df.drop(['Unnamed: 2','Unnamed: 3', 'Unnamed: 4'], axis=1)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa83598b",
   "metadata": {},
   "source": [
    "The first column of our dataset identifies if it is a spam text. We used label encoding below to change the column to be 0 for a normal text and 1 for a spam text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03f7e0c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   v1                                                 v2\n",
       "0   0  Go until jurong point, crazy.. Available only ...\n",
       "1   0                      Ok lar... Joking wif u oni...\n",
       "2   1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   0  U dun say so early hor... U c already then say...\n",
       "4   0  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(df['v1'])\n",
    "df['v1'] = le.transform(df['v1'])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0af1766",
   "metadata": {},
   "source": [
    "Looking at the totals for each category, we can see that there are many more normal text messages than spam messages in our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5991375",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4825\n",
       "1     747\n",
       "Name: v1, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totals = df['v1'].value_counts()\n",
    "totals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fbb2d2",
   "metadata": {},
   "source": [
    "The longest text message in our dataset is 171 which we will use for our max length during tokenization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0e71155",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "171"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_words = 0\n",
    "for i in df.v2:\n",
    "    if len(i.split()) > max_words:\n",
    "        max_words = len(i.split())\n",
    "max_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6187c0",
   "metadata": {},
   "source": [
    "For our tokenization, we converted each word to an integer using the Keras Tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06fa4d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8920 unique tokens. Distilled to 8920 top words.\n",
      "Shape of data tensor: (5572, 171)\n",
      "CPU times: total: 13.1 s\n",
      "Wall time: 27.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "NUM_TOP_WORDS = None # use entire vocabulary!\n",
    "MAX_ART_LEN = max_words # maximum and minimum number of words\n",
    "\n",
    "#tokenize the text\n",
    "tokenizer = Tokenizer(num_words=NUM_TOP_WORDS)\n",
    "tokenizer.fit_on_texts(df.v2)\n",
    "# save as sequences with integers replacing words\n",
    "sequences = tokenizer.texts_to_sequences(df.v2)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "NUM_TOP_WORDS = len(word_index) if NUM_TOP_WORDS==None else NUM_TOP_WORDS\n",
    "top_words = min((len(word_index),NUM_TOP_WORDS))\n",
    "print('Found %s unique tokens. Distilled to %d top words.' % (len(word_index),top_words))\n",
    "\n",
    "X = pad_sequences(sequences, maxlen=MAX_ART_LEN)\n",
    "print('Shape of data tensor:', X.shape)\n",
    "y = df.v1.astype(np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd61fa95",
   "metadata": {},
   "source": [
    "For our business case, we want to notify users when they recieve a text that may be spam. If we misclassify something as spam, the user may be slightly annoyed. However, if we do not clasify something as spam when it is, it could lead to the user trusting a fraudulent text. Therefore we want to minimize the number of false negative (number of spam texts classified as normal texts), so we will use recall to measure our algorithm's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab58da1a",
   "metadata": {},
   "source": [
    "We chose to use a Stratified 10-fold cross validation for dividing our data into training and testing. We chose this method because our dataset is smaller (5572) and there are not many examples of spam texts (747). We want to make sure that we represent the entire dataset in equal proportion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15b083d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1)\n",
    "y_train = y_train.values.astype(np.int32)\n",
    "y_test = y_test.values.astype(np.int32)\n",
    "kfold = StratifiedKFold(n_splits=10).split(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542cad62",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ca36fa",
   "metadata": {},
   "source": [
    "For our modeling, we decided to investigate LSTM and GRU recurrent network architectures. To start, we used GloVe pre-trained embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7bb7789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n",
      "Embedding Shape: (8921, 100) \n",
      " Total words found: 6518 \n",
      " Percentage: 73.06355789709674\n",
      "CPU times: total: 36.9 s\n",
      "Wall time: 43.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "EMBED_SIZE = 100\n",
    "# the embed size should match the file you load glove from\n",
    "embeddings_index = {}\n",
    "f = open('large_data/glove.6b/glove.6B.100d.txt', encoding=\"utf8\")\n",
    "# save key/array pairs of the embeddings\n",
    "#  the key of the dictionary is the word, the array is the embedding\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "# now fill in the matrix, using the ordering from the\n",
    "#  keras word tokenizer from before\n",
    "found_words = 0\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBED_SIZE))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be ALL-ZEROS\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        found_words = found_words+1\n",
    "\n",
    "print(\"Embedding Shape:\",embedding_matrix.shape, \"\\n\",\n",
    "      \"Total words found:\",found_words, \"\\n\",\n",
    "      \"Percentage:\",100*found_words/embedding_matrix.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbfd30ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Input, LSTM, GRU\n",
    "from tensorflow.keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07ecd657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save this embedding now\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBED_SIZE,\n",
    "                            weights=[embedding_matrix],# here is the embedding getting saved\n",
    "                            input_length=MAX_ART_LEN,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce2be94",
   "metadata": {},
   "source": [
    "Our first two models are LSTM models. We adjusted the hyper-parameters for the recurrent dropout for our second model. In our first model, the reccurent dropout is 0.2, while in the second model it is 0.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1bd4b143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 171, 100)          892100    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 100)               80400     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 972,601\n",
      "Trainable params: 80,501\n",
      "Non-trainable params: 892,100\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "lstm1 = Sequential()\n",
    "lstm1.add(embedding_layer)\n",
    "lstm1.add(LSTM(100,dropout=0.2, recurrent_dropout=0.2))\n",
    "lstm1.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "print(lstm1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9db12c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 171, 100)          892100    \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 100)               80400     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 972,601\n",
      "Trainable params: 80,501\n",
      "Non-trainable params: 892,100\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "lstm2 = Sequential()\n",
    "lstm2.add(embedding_layer)\n",
    "lstm2.add(LSTM(100,dropout=0.2, recurrent_dropout=0.3))\n",
    "lstm2.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "print(lstm2.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe677ed",
   "metadata": {},
   "source": [
    "Our last two models are GRU models. We adjusted the hyper-parameters for the recurrent dropout for our fourth model. In our thrid model, the reccurent dropout is 0.2, while in the fourth model it is 0.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b9871c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 171, 100)          892100    \n",
      "                                                                 \n",
      " gru (GRU)                   (None, 100)               60600     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 952,801\n",
      "Trainable params: 60,701\n",
      "Non-trainable params: 892,100\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "gru1 = Sequential()\n",
    "gru1.add(embedding_layer)\n",
    "gru1.add(GRU(100,dropout=0.2, recurrent_dropout=0.2))\n",
    "gru1.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "print(gru1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76950efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 171, 100)          892100    \n",
      "                                                                 \n",
      " gru_1 (GRU)                 (None, 100)               60600     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 952,801\n",
      "Trainable params: 60,701\n",
      "Non-trainable params: 892,100\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "gru2 = Sequential()\n",
    "gru2.add(embedding_layer)\n",
    "gru2.add(GRU(100,dropout=0.2, recurrent_dropout=0.3))\n",
    "gru2.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "print(gru2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e8e05aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "\n",
    "def roc(model, test, train, mean_tpr):\n",
    "    probas = model.predict(X_train[test], verbose=0)\n",
    "    fpr, tpr, thresholds = roc_curve(y_train[test], probas, pos_label=1)\n",
    "    mean_tpr += np.interp(mean_fpr, fpr, tpr)\n",
    "    mean_tpr[0] = 0.0\n",
    "    return mean_tpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3e7c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for fold 1 ...\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "acc_per_fold1 = []\n",
    "recall_per_fold1 = []\n",
    "loss_per_fold1 = []\n",
    "mean_tpr1 = 0.0\n",
    "\n",
    "acc_per_fold2 = []\n",
    "recall_per_fold2 = []\n",
    "loss_per_fold2 = []\n",
    "mean_tpr2 = 0.0\n",
    "\n",
    "acc_per_fold3 = []\n",
    "recall_per_fold3 = []\n",
    "loss_per_fold3 = []\n",
    "mean_tpr3 = 0.0\n",
    "\n",
    "acc_per_fold4 = []\n",
    "recall_per_fold4 = []\n",
    "loss_per_fold4 = []\n",
    "mean_tpr4 = 0.0\n",
    "\n",
    "# K-fold Cross Validation model evaluation\n",
    "for k, (train, test) in enumerate(kfold):\n",
    "\n",
    "    # Compile the model\n",
    "    lstm1.compile(loss='binary_crossentropy',\n",
    "            optimizer='rmsprop',\n",
    "            metrics=['accuracy', keras.metrics.Recall()])\n",
    "    lstm2.compile(loss='binary_crossentropy',\n",
    "            optimizer='rmsprop',\n",
    "            metrics=['accuracy', keras.metrics.Recall()])\n",
    "    gru1.compile(loss='binary_crossentropy',\n",
    "            optimizer='rmsprop',\n",
    "            metrics=['accuracy', keras.metrics.Recall()])\n",
    "    gru2.compile(loss='binary_crossentropy',\n",
    "            optimizer='rmsprop',\n",
    "            metrics=['accuracy', keras.metrics.Recall()])\n",
    "\n",
    "\n",
    "    # Generate a print\n",
    "    print(f'Training for fold {k+1} ...')\n",
    "\n",
    "    # Fit data to model\n",
    "    history1 = lstm1.fit(X_train[train], y_train[train], epochs=3, batch_size=64, validation_data=(X_train[test], y_train[test],), verbose=0)\n",
    "    history2 = lstm2.fit(X_train[train], y_train[train], epochs=3, batch_size=64, validation_data=(X_train[test], y_train[test],), verbose=0)\n",
    "    history3 = gru1.fit(X_train[train], y_train[train], epochs=3, batch_size=64, validation_data=(X_train[test], y_train[test],), verbose=0)\n",
    "    history4 = gru2.fit(X_train[train], y_train[train], epochs=3, batch_size=64, validation_data=(X_train[test], y_train[test],), verbose=0)\n",
    "\n",
    "    mean_tpr1 = roc(lstm1, test, train, mean_tpr1)\n",
    "    mean_tpr2 = roc(lstm2, test, train, mean_tpr2)\n",
    "    mean_tpr3 = roc(gru1, test, train, mean_tpr3)\n",
    "    mean_tpr4 = roc(gru2, test, train, mean_tpr4)\n",
    "\n",
    "    # Generate generalization metrics\n",
    "    scores1 = lstm1.evaluate(X_train[test], y_train[test], verbose=0)\n",
    "    scores2 = lstm2.evaluate(X_train[test], y_train[test], verbose=0)\n",
    "    scores3 = gru1.evaluate(X_train[test], y_train[test], verbose=0)\n",
    "    scores4 = gru2.evaluate(X_train[test], y_train[test], verbose=0)\n",
    "\n",
    "    print(f'Score for LSTM1: {lstm1.metrics_names[0]} of {scores1[0]}; {lstm1.metrics_names[1]} of {scores1[1]*100}%; {lstm1.metrics_names[2]} of {scores1[2]*100}%')\n",
    "    print(f'Score for LSTM2: {lstm2.metrics_names[0]} of {scores2[0]}; {lstm2.metrics_names[1]} of {scores2[1]*100}%; {lstm2.metrics_names[2]} of {scores2[2]*100}%')\n",
    "    print(f'Score for GRU1: {gru1.metrics_names[0]} of {scores3[0]}; {gru1.metrics_names[1]} of {scores3[1]*100}%; {gru1.metrics_names[2]} of {scores3[2]*100}%')\n",
    "    print(f'Score for GRU2: {gru2.metrics_names[0]} of {scores4[0]}; {gru2.metrics_names[1]} of {scores4[1]*100}%; {gru2.metrics_names[2]} of {scores4[2]*100}%')\n",
    "\n",
    "    loss_per_fold1.append(scores1[0])\n",
    "    acc_per_fold1.append(scores1[1] * 100)\n",
    "    recall_per_fold1.append(scores1[2]*100)\n",
    "    loss_per_fold2.append(scores2[0])\n",
    "    acc_per_fold2.append(scores2[1] * 100)\n",
    "    recall_per_fold2.append(scores2[2]*100)\n",
    "    loss_per_fold3.append(scores3[0])\n",
    "    acc_per_fold3.append(scores3[1] * 100)\n",
    "    recall_per_fold3.append(scores3[2]*100)\n",
    "    loss_per_fold4.append(scores4[0])\n",
    "    acc_per_fold4.append(scores4[1] * 100)\n",
    "    recall_per_fold4.append(scores4[2]*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f568fc2",
   "metadata": {},
   "source": [
    "**Discuss performance of models. Visualize the results of all the RNNs you trained.  Use proper statistical comparison techniques to determine which method(s) is (are) superior.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
